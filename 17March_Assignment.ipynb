{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e8b176-affa-4a33-bb00-703a820a1b3c",
   "metadata": {},
   "source": [
    "# 1Ans:\n",
    "Missing values refer to the absence of a value in a particular observation or record in a dataset. There are many reasons why data may be missing, such as human error, equipment failure, or a faulty data collection process.\n",
    "\n",
    "It is essential to handle missing values because they can lead to biased or inaccurate analysis, as well as reduce the effectiveness of machine learning algorithms. The presence of missing values in a dataset can impact the statistical power of a model, which can cause inaccurate estimates and reduce the effectiveness of predictive models.\n",
    "\n",
    "Some algorithms that are not affected by missing values include decision trees, random forests, and gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf2c30-54d1-4a66-89e4-a788dd57fdb7",
   "metadata": {},
   "source": [
    "# 2ans:\n",
    "\n",
    "There are several techniques that can be used to handle missing data in a dataset. Here are some of the commonly used techniques along with an example of each using Python:\n",
    "\n",
    "Deletion: This technique involves removing the observations with missing values from the dataset. There are two methods of deletion: Listwise Deletion and Pairwise Deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152eea4b-c8fd-4eed-8369-8f96d8d1a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "1  2.0  NaN  10\n",
      "2  NaN  NaN  11\n",
      "3  4.0  8.0  12\n",
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n",
      "    C\n",
      "0   9\n",
      "1  10\n",
      "2  11\n",
      "3  12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# creating a sample dataset\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "print(df)\n",
    "\n",
    "# Listwise Deletion\n",
    "df1 = df.dropna()\n",
    "print(df1)\n",
    "\n",
    "# Pairwise Deletion\n",
    "df2 = df.dropna(axis=1, how='any')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e4dac-67f9-4042-9d99-5a05e3f4c997",
   "metadata": {},
   "source": [
    "Imputation: This technique involves replacing missing values with estimates of the missing values. The most commonly used imputation techniques are mean imputation, median imputation, and mode imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374202aa-14ca-4857-92b8-79bc7c2f925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "1  2.0  NaN  10\n",
      "2  NaN  NaN  11\n",
      "3  4.0  8.0  12\n",
      "          A    B     C\n",
      "0  1.000000  5.0   9.0\n",
      "1  2.000000  6.5  10.0\n",
      "2  2.333333  6.5  11.0\n",
      "3  4.000000  8.0  12.0\n",
      "     A    B     C\n",
      "0  1.0  5.0   9.0\n",
      "1  2.0  6.5  10.0\n",
      "2  2.0  6.5  11.0\n",
      "3  4.0  8.0  12.0\n",
      "     A    B     C\n",
      "0  1.0  5.0   9.0\n",
      "1  2.0  5.0  10.0\n",
      "2  1.0  5.0  11.0\n",
      "3  4.0  8.0  12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# creating a sample dataset\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "print(df)\n",
    "\n",
    "# Mean Imputation\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df1 = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df1)\n",
    "\n",
    "# Median Imputation\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "df2 = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df2)\n",
    "\n",
    "# Mode Imputation\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "df3 = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55566d73-121f-4aec-a9e6-ee41aba562c2",
   "metadata": {},
   "source": [
    "# 3ans:\n",
    "\n",
    "imbalanced data refers to a situation where the number of observations in one class or category is significantly higher than the number of observations in another class or category.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to biased or inaccurate predictive models. Since machine learning models are designed to minimize error, they will typically be biased towards the majority class and under-predict the minority class. In other words, the model will predict the majority class more often, leading to poor performance on the minority class. This can result in false negatives and false positives, which can be especially problematic in situations where the minority class is critical or has a high cost of misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca803828-a5c4-4070-8a4c-5297e79510bd",
   "metadata": {},
   "source": [
    "# 4ans:\n",
    "\n",
    "Up-sampling and down-sampling are two common techniques used to handle imbalanced datasets.\n",
    "\n",
    "Up-sampling involves increasing the number of samples in the minority class to balance the dataset with the majority class. This can be done by replicating existing samples in the minority class or by generating synthetic samples using techniques like Synthetic Minority Over-sampling Technique (SMOTE).\n",
    "\n",
    "Down-sampling involves reducing the number of samples in the majority class to balance the dataset with the minority class. This can be done by randomly selecting a subset of samples from the majority class\n",
    "\n",
    "Here is an example to illustrate when up-sampling and down-sampling may be required:\n",
    "\n",
    "Suppose we have a binary classification problem where we are trying to predict whether a credit card transaction is fraudulent or not. In our dataset, we have 10,000 transactions, out of which only 100 are fraudulent (positive class), while the remaining 9,900 are legitimate (negative class). This is an imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7253e4-6674-44fa-8749-1778a20f2932",
   "metadata": {},
   "source": [
    "# 5ans:\n",
    "\n",
    "Data augmentation is a technique used to increase the size and diversity of a dataset by creating new samples that are similar to existing ones. This technique is commonly used in machine learning to address problems such as overfitting, imbalanced datasets, and limited training data.\n",
    "\n",
    "SMOTE can be an effective technique for improving the performance of machine learning models on imbalanced datasets, but it is essential to use it appropriately and evaluate the performance of the model on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b1c1b-af1c-4c3e-9b8d-ee972d20d3c9",
   "metadata": {},
   "source": [
    "# 6ans:\n",
    "\n",
    "Outliers are data points that are significantly different from other observations in a dataset. These observations are usually located far away from the majority of the data points and can distort statistical analyses, machine learning models, and data visualization.\n",
    "\n",
    "It is essential to handle outliers in a dataset because they can have a significant impact on the analysis and interpretation of data. Outliers can distort statistical measures such as the mean, standard deviation, and correlation coefficients, leading to biased results. They can also have a significant impact on machine learning models by influencing the training process, leading to overfitting or underfitting, and reducing the performance of the model on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be67b9a-172e-4df8-9fc6-62c84cb38de4",
   "metadata": {},
   "source": [
    "# 7ans:\n",
    "\n",
    "There are several techniques that can be used to handle missing data in customer data analysis. Some of these techniques are:\n",
    "\n",
    "Deleting missing data: This technique involves removing the missing data from the dataset. This approach is only recommended if the missing data is relatively small, and deleting it does not significantly affect the analysis.\n",
    "\n",
    "Mean/median imputation: In this technique, the missing values are replaced with the mean or median value of the variable. This approach assumes that the missing values are missing at random (MAR) and that the non-missing values are representative of the overall distribution of the variable.\n",
    "\n",
    "Using machine learning algorithms: Machine learning algorithms can handle missing data by using algorithms that are designed to handle missing values. For example, decision trees and random forests can handle missing data by using surrogate spli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a1ffa-3798-4247-99f9-15ed5dd8700d",
   "metadata": {},
   "source": [
    "# 8ans:\n",
    "\n",
    "Determining if the missing data is missing at random or if there is a pattern to the missing data is crucial in data analysis, as it can affect the validity and reliability of the results. Here are some strategies to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "Visual inspection: One way to check for patterns in missing data is to use visual inspection. For example, you can create a missing data plot, which shows the distribution of missing values across the dataset. If there is a pattern to the missing data, it may be visible in the plot\n",
    "\n",
    "Statistical tests: There are several statistical tests that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bda220-928a-458f-8d41-a17971402a7a",
   "metadata": {},
   "source": [
    "# 9ans:\n",
    "\n",
    "Here are some strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "Confusion matrix: A confusion matrix is a table that shows the true positive, false positive, true negative, and false negative rates of a model. It can help evaluate the performance of the model on an imbalanced dataset and identify any biases or errors in the predictions.\n",
    "\n",
    "ROC curve and AUC: A Receiver Operating Characteristic (ROC) curve plots the true positive rate against the false positive rate, and the Area Under the Curve (AUC) measures the performance of the model. ROC and AUC can help evaluate the performance of the model on an imbalanced dataset and provide a threshold for decision making.\n",
    "\n",
    "Precision-Recall curve: The Precision-Recall (PR) curve plots the precision and recall rates of the model. It is useful when the positive class is rare, as it focuses on the performance of the model in predicting the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6602dbe-63d9-45bf-88e7-d8aafc71eba8",
   "metadata": {},
   "source": [
    "# 10ans:\n",
    "\n",
    "To balance an unbalanced dataset where the majority class dominates the dataset, there are several methods that can be employed. Here are some common methods:\n",
    "\n",
    "Down-sampling the majority class: Down-sampling involves randomly removing some samples from the majority class to balance the dataset. This can be achieved by randomly selecting a subset of the majority class that is equal in size to the minority class. The drawback of this approach is that it can result in loss of information.\n",
    "\n",
    "Up-sampling the minority class: Up-sampling involves randomly replicating samples from the minority class to balance the dataset. This can be achieved by replicating the minority class samples until it is equal in size to the majority class. The drawback of this approach is that it can result in overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8cd40-5a40-4c55-b52f-8c86987a0608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
